{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5b0ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading Model from: c:/Users/Rayan/Downloads/Compressed/unzipped_files-20260214T145034Z-1-001/unzipped_files/submission1/best_model_unetpp.pth\n",
      "ðŸ“‚ Found 738 Test Images in 'test_public_80'\n",
      "âš¡ Running Inference on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 564/738 [15:35<04:48,  1.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 198\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“ Results saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_CONFIG[\u001b[33m'\u001b[39m\u001b[33mOUTPUT_DIR\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# Run It\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[43mrun_test_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mrun_test_evaluation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# 1. Inference Speed\u001b[39;00m\n\u001b[32m    137\u001b[39m start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m end = time.time()\n\u001b[32m    140\u001b[39m inference_times.append((end - start) * \u001b[32m1000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:66\u001b[39m, in \u001b[36mSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m     62\u001b[39m     torch.jit.is_scripting() \u001b[38;5;129;01mor\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mor\u001b[39;00m is_torch_compiling()\n\u001b[32m     63\u001b[39m ):\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_input_shape(x)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m decoder_output = \u001b[38;5;28mself\u001b[39m.decoder(features)\n\u001b[32m     69\u001b[39m masks = \u001b[38;5;28mself\u001b[39m.segmentation_head(decoder_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\encoders\\efficientnet.py:77\u001b[39m, in \u001b[36mEfficientNetEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._blocks):\n\u001b[32m     76\u001b[39m     drop_connect_prob = \u001b[38;5;28mself\u001b[39m._drop_connect_rate * i / \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._blocks)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_connect_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._out_indexes:\n\u001b[32m     80\u001b[39m         features.append(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\encoders\\_efficientnet.py:189\u001b[39m, in \u001b[36mMBConvBlock.forward\u001b[39m\u001b[34m(self, inputs, drop_connect_rate)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Pointwise Convolution\u001b[39;00m\n\u001b[32m    188\u001b[39m x = \u001b[38;5;28mself\u001b[39m._project_conv(x)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bn2\u001b[49m(x)\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Skip connection and drop connect\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_drop_connect:\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# The combination of skip connection and drop connect brings about stochastic depth.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1951\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1946\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1953\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# FINAL TEST SET EVALUATION (test_public_80)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TEST_CONFIG = {\n",
    "    \"TEST_DIR\": r\"c:/Users/Rayan/Downloads/Compressed/unzipped_files-20260214T145034Z-1-001/unzipped_files/test_public_80\",\n",
    "    \"MODEL_PATH\": r\"c:/Users/Rayan/Downloads/Compressed/unzipped_files-20260214T145034Z-1-001/unzipped_files/submission1/best_model_unetpp.pth\",\n",
    "    \"OUTPUT_DIR\": \"./test_results_public_80\",\n",
    "    \"IMG_SIZE\": 512,\n",
    "    \"ENCODER\": \"efficientnet-b4\",\n",
    "    \"NUM_CLASSES\": 10,\n",
    "    \"DEVICE\": torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "# Visualization Colors (BGR)\n",
    "COLOR_MAP = {\n",
    "    0: (0, 0, 0),       1: (34, 139, 34),   2: (0, 255, 0),     3: (140, 180, 210),\n",
    "    4: (43, 90, 139),   5: (0, 128, 128),   6: (19, 69, 139),   7: (128, 128, 128),\n",
    "    8: (45, 82, 160),   9: (235, 206, 135)\n",
    "}\n",
    "CLASS_NAMES = ['Background', 'Trees', 'Lush Bushes', 'Dry Grass', 'Dry Bushes',\n",
    "               'Ground Clutter', 'Logs', 'Rocks', 'Landscape', 'Sky']\n",
    "\n",
    "ID_TO_INDEX = {\n",
    "    100: 0, 200: 1, 300: 2, 500: 3, 550: 4,\n",
    "    600: 5, 700: 6, 800: 7, 7100: 8, 10000: 9\n",
    "}\n",
    "\n",
    "# --- DATASET LOADER ---\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.image_dir = os.path.join(root_dir, 'Color_Images')\n",
    "        # Handle case sensitivity\n",
    "        p1 = os.path.join(root_dir, 'segmentation')\n",
    "        p2 = os.path.join(root_dir, 'Segmentation')\n",
    "        self.masks_dir = p2 if os.path.exists(p2) else p1\n",
    "\n",
    "        self.images = sorted(glob.glob(os.path.join(self.image_dir, \"*.*\")))\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(TEST_CONFIG['IMG_SIZE'], TEST_CONFIG['IMG_SIZE']),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        # Verify\n",
    "        if len(self.images) == 0:\n",
    "            print(f\"âŒ CRITICAL ERROR: No images found in {self.image_dir}\")\n",
    "\n",
    "    def __len__(self): return len(self.images)\n",
    "\n",
    "    def map_mask(self, mask):\n",
    "        new_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "        for k, v in ID_TO_INDEX.items(): new_mask[mask == k] = v\n",
    "        return new_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        filename = os.path.basename(img_path)\n",
    "        mask_path = os.path.join(self.masks_dir, os.path.splitext(filename)[0] + \".png\")\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Try loading mask\n",
    "        if os.path.exists(mask_path):\n",
    "            mask = cv2.imread(mask_path, -1)\n",
    "            mask = self.map_mask(mask)\n",
    "        else:\n",
    "            # If no mask (blind test), return zeros\n",
    "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        aug = self.transform(image=image, mask=mask)\n",
    "        return aug['image'], aug['mask'].long(), image, filename\n",
    "\n",
    "# --- METRIC CALCULATION ---\n",
    "def compute_iou(pred, target, num_classes=10):\n",
    "    pred = torch.argmax(pred, dim=1).view(-1)\n",
    "    target = target.view(-1)\n",
    "    iou_list = []\n",
    "    # Ignore background class 0 if needed, but usually included\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = (pred_inds | target_inds).sum().float()\n",
    "\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan')) # Class not present in this image\n",
    "        else:\n",
    "            iou_list.append((intersection / union).item())\n",
    "    return np.nanmean(iou_list), iou_list\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "def run_test_evaluation():\n",
    "    print(f\"ðŸš€ Loading Model from: {TEST_CONFIG['MODEL_PATH']}\")\n",
    "    if not os.path.exists(TEST_CONFIG['MODEL_PATH']):\n",
    "        print(\"âŒ Error: Model file missing.\")\n",
    "        return\n",
    "\n",
    "    # Load Model\n",
    "    model = smp.UnetPlusPlus(\n",
    "        encoder_name=TEST_CONFIG['ENCODER'], in_channels=3, classes=TEST_CONFIG['NUM_CLASSES']\n",
    "    ).to(TEST_CONFIG['DEVICE'])\n",
    "    model.load_state_dict(torch.load(TEST_CONFIG['MODEL_PATH'], map_location=TEST_CONFIG['DEVICE']))\n",
    "    model.eval()\n",
    "\n",
    "    # Load Data\n",
    "    ds = TestDataset(TEST_CONFIG['TEST_DIR'])\n",
    "    loader = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "    print(f\"ðŸ“‚ Found {len(ds)} Test Images in 'test_public_80'\")\n",
    "\n",
    "    os.makedirs(TEST_CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
    "    viz_dir = os.path.join(TEST_CONFIG['OUTPUT_DIR'], \"visualizations\")\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "    inference_times = []\n",
    "    all_ious = []\n",
    "    class_ious = {i: [] for i in range(TEST_CONFIG['NUM_CLASSES'])}\n",
    "\n",
    "    print(\"âš¡ Running Inference on Test Set...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (img_t, mask_t, original_img, fname) in enumerate(tqdm(loader)):\n",
    "            img_t, mask_t = img_t.to(TEST_CONFIG['DEVICE']), mask_t.to(TEST_CONFIG['DEVICE'])\n",
    "\n",
    "            # 1. Inference Speed\n",
    "            start = time.time()\n",
    "            logits = model(img_t)\n",
    "            end = time.time()\n",
    "            inference_times.append((end - start) * 1000)\n",
    "\n",
    "            # 2. Metrics\n",
    "            # Note: We only calculate IoU if the mask is not all zeros (valid ground truth)\n",
    "            if mask_t.max() > 0 or mask_t.sum() > 0:\n",
    "                mean_iou, cls_iou_list = compute_iou(logits, mask_t, TEST_CONFIG['NUM_CLASSES'])\n",
    "                all_ious.append(mean_iou)\n",
    "                for cls_idx, val in enumerate(cls_iou_list):\n",
    "                    if not np.isnan(val): class_ious[cls_idx].append(val)\n",
    "\n",
    "            # 3. Visualizations (Save first 10 + any with specific filename if you want)\n",
    "            if i < 10:\n",
    "                pred_mask = torch.argmax(logits, dim=1).squeeze().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "                # Create Color Mask\n",
    "                h, w = pred_mask.shape\n",
    "                viz_pred = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "                for k, v in COLOR_MAP.items(): viz_pred[pred_mask == k] = v\n",
    "\n",
    "                # Resize original to 512 for stacking\n",
    "                orig_rez = cv2.resize(original_img[0].numpy(), (512, 512))\n",
    "\n",
    "                # Stack: Original | Prediction\n",
    "                viz_final = np.hstack([orig_rez, viz_pred])\n",
    "\n",
    "                save_path = os.path.join(viz_dir, f\"pred_{fname[0]}\")\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(viz_final, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # --- REPORT GENERATION ---\n",
    "    avg_speed = np.mean(inference_times)\n",
    "    final_mIoU = np.nanmean(all_ious) if all_ious else 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"ðŸ† TEST SET RESULTS (test_public_80)\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"âœ… Mean IoU (mIoU):  {final_mIoU:.4f}\")\n",
    "    print(f\"âš¡ Avg Inference:    {avg_speed:.2f} ms\")\n",
    "\n",
    "    # Per-Class Table\n",
    "    res_data = []\n",
    "    for i in range(TEST_CONFIG['NUM_CLASSES']):\n",
    "        score = np.mean(class_ious[i]) if class_ious[i] else 0.0\n",
    "        res_data.append({\"Class\": CLASS_NAMES[i], \"IoU\": score})\n",
    "\n",
    "    df = pd.DataFrame(res_data)\n",
    "    print(\"\\nðŸ“Š Per-Class Breakdown:\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # Save Report\n",
    "    with open(f\"{TEST_CONFIG['OUTPUT_DIR']}/final_test_report.txt\", \"w\") as f:\n",
    "        f.write(f\"Test Set Evaluation\\n\")\n",
    "        f.write(f\"Mean IoU: {final_mIoU:.4f}\\n\")\n",
    "        f.write(f\"Speed: {avg_speed:.2f} ms\\n\\n\")\n",
    "        f.write(df.to_string())\n",
    "\n",
    "    print(f\"\\nðŸ“ Results saved to: {TEST_CONFIG['OUTPUT_DIR']}\")\n",
    "\n",
    "# Run It\n",
    "run_test_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b2438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
